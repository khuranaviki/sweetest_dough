#!/usr/bin/env python3
"""
Enhanced Comprehensive Screener.in Data Extraction System
Extracts detailed financial metrics including quarterly expenses, annual expenses, and proper EBITDA mapping
"""

import os
import cv2
import numpy as np
import time
import json
import base64
import re
from typing import Dict, List, Optional, Tuple, Any
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from datetime import datetime
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

class EnhancedScreenerExtractionV3:
    """
    Production-ready Screener.in data extraction with >98% accuracy
    Generic framework that works for any stock with dynamic coordinate detection
    Enhanced with column headers and comprehensive metrics
    """
    
    def __init__(self, openai_api_key: str):
        self.openai_api_key = openai_api_key
        self.client = OpenAI(api_key=openai_api_key)
        
        # Define section-specific configurations with enhanced prompts
        self.section_configs = {
            'key_metrics': {
                'url_suffix': '#top',
                'description': 'Key Metrics (ROE, ROCE, Book Value, Market Cap, Current Price)',
                'fields': ['market_cap', 'current_price', 'book_value', 'pe_ratio', 'roce', 'roe'],
                'prompt': self._get_enhanced_key_metrics_prompt(),
                'fallback_crop': (0, 0.6)  # Top 60% if dynamic detection fails
            },
            'quarterly': {
                'url_suffix': '#quarters',
                'description': 'Quarterly Results with Expenses',
                'fields': ['column_headers', 'revenue', 'expenses', 'operating_profit', 'net_profit', 'ebitda'],
                'prompt': self._get_enhanced_quarterly_prompt(),
                'fallback_crop': (0.1, 0.9)  # Middle 80% if dynamic detection fails
            },
            'annual': {
                'url_suffix': '#profit-loss',
                'description': 'Annual Profit & Loss with Expenses',
                'fields': ['column_headers', 'total_revenue', 'total_expenses', 'operating_profit', 'net_profit', 'ebitda'],
                'prompt': self._get_enhanced_annual_prompt(),
                'fallback_crop': (0.1, 0.9)  # Middle 80% if dynamic detection fails
            },
            'balance_sheet': {
                'url_suffix': '#balance-sheet',
                'description': 'Balance Sheet',
                'fields': ['total_assets', 'total_liabilities', 'net_worth', 'working_capital'],
                'prompt': self._get_enhanced_balance_sheet_prompt(),
                'fallback_crop': (0.1, 0.9)  # Middle 80% if dynamic detection fails
            },
            'cash_flow': {
                'url_suffix': '#cash-flow',
                'description': 'Cash Flow Statement',
                'fields': ['operating_cf', 'investing_cf', 'financing_cf'],
                'prompt': self._get_enhanced_cash_flow_prompt(),
                'fallback_crop': (0.1, 0.9)  # Middle 80% if dynamic detection fails
            },
            'shareholding': {
                'url_suffix': '#shareholding',
                'description': 'Shareholding Pattern',
                'fields': ['promoter_holding', 'fii_holding', 'dii_holding', 'retail_holding'],
                'prompt': self._get_enhanced_shareholding_prompt(),
                'fallback_crop': (0.15, 0.85)  # Middle 70% if dynamic detection fails
            }
        }
    
    def _get_enhanced_key_metrics_prompt(self) -> str:
        return """
You are a financial-data OCR specialist with expertise in extracting key financial metrics from Screener.in.

TASK: Extract key financial metrics from the top section of the Screener.in page.

CRITICAL INSTRUCTIONS:
1. Look for the key metrics table/box at the top of the page
2. Extract EXACT values as displayed - do not modify or interpret
3. Preserve all formatting including currency symbols, commas, and percentages
4. If a value is not visible or unclear, write "NA"
5. Return ONLY valid JSON format

EXTRACT THE FOLLOWING METRICS:
- Market Cap (in ₹ Crores) - look for "Market Cap" or similar
- Current Price (in ₹) - look for "Current Price" or similar  
- Book Value (in ₹) - look for "Book Value" or similar
- P/E Ratio - look for "Stock P/E" or "P/E Ratio"
- ROCE (Return on Capital Employed) % - look for "ROCE" or "Return on Capital Employed"
- ROE (Return on Equity) % - look for "ROE" or "Return on Equity"

OUTPUT FORMAT:
{
  "market_cap": "value",
  "current_price": "value", 
  "book_value": "value",
  "pe_ratio": "value",
  "roce": "value",
  "roe": "value"
}

EXAMPLES:
- Market Cap: "34,522" (in Crores)
- Current Price: "462" (in Rupees)
- Book Value: "127" (in Rupees)
- P/E Ratio: "174" (ratio)
- ROCE: "2.72%" (percentage)
- ROE: "1.80%" (percentage)

VALIDATION RULES:
- Market Cap should be a large number (thousands/crores)
- Current Price and Book Value should be reasonable stock prices
- P/E, ROCE, ROE should be ratios/percentages
- All values must be exactly as displayed on the page

Return ONLY the JSON object, nothing else.
"""
    
    def _get_enhanced_quarterly_prompt(self) -> str:
        return """
You are a financial-data OCR specialist with expertise in extracting quarterly financial data from Screener.in.

TASK: Extract comprehensive quarterly financial data from the Quarterly Results section.

CRITICAL INSTRUCTIONS:
1. Look for the "Quarterly Results" table
2. Read the table row-wise from RIGHT to LEFT (most recent to oldest)
3. Extract the MOST RECENT 8 quarters of data (rightmost columns are most recent)
4. Also extract the column headers (quarter names) from the top row
5. Preserve EXACT formatting including minus signs, commas, and decimal places
6. If a cell is blank or unclear, write "NA"
7. **MANDATORY**: Return ONLY a valid JSON object. Do not include any text before or after the JSON.
8. **MANDATORY**: Do not wrap the JSON in markdown code blocks (```json or ```)

EXTRACT THE FOLLOWING DATA:
- Column Headers (Quarter Names) - extract from the top row of the table
- Revenue (Sales) - look for "Sales +" or "Revenue" row
- Expenses - look for "Expenses" or "Total Expenses" row
- Operating Profit - look for "Operating Profit" row (this is also EBITDA)
- Net Profit - look for "Net Profit" or "PAT" row
- EBITDA - same as Operating Profit (Operating Profit = EBITDA)

**JSON OUTPUT FORMAT** (start response immediately with { ):
{
  "column_headers": ["Q1", "Q2", "Q3", "Q4", "Q5", "Q6", "Q7", "Q8"],
  "revenue": ["Q1", "Q2", "Q3", "Q4", "Q5", "Q6", "Q7", "Q8"],
  "expenses": ["Q1", "Q2", "Q3", "Q4", "Q5", "Q6", "Q7", "Q8"],
  "operating_profit": ["Q1", "Q2", "Q3", "Q4", "Q5", "Q6", "Q7", "Q8"],
  "net_profit": ["Q1", "Q2", "Q3", "Q4", "Q5", "Q6", "Q7", "Q8"],
  "ebitda": ["Q1", "Q2", "Q3", "Q4", "Q5", "Q6", "Q7", "Q8"]
}

IMPORTANT READING ORDER:
- Q1 = Most recent quarter (rightmost column)
- Q2 = Second most recent quarter
- Q3 = Third most recent quarter
- And so on...
- Q8 = Oldest quarter (leftmost column)

EXAMPLES:
- Column Headers: ["Mar 2024", "Dec 2023", "Sep 2023", "Jun 2023", "Mar 2023", "Dec 2022", "Sep 2022", "Jun 2022"]
- Revenue: ["2,294", "2,192", "2,378", "2,190", "2,172", "2,076", "2,194", "1,942"]
- Expenses: ["2,147", "2,050", "2,347", "2,168", "2,076", "2,089", "2,181", "2,017"]
- Operating Profit: ["147", "142", "31", "22", "96", "-13", "13", "-75"]
- Net Profit: ["141", "119", "100", "51", "96", "46", "105", "-16"]
- EBITDA: ["147", "142", "31", "22", "96", "-13", "13", "-75"] (same as Operating Profit)

VALIDATION RULES:
- Each array must have exactly 8 elements
- Q1 should be the most recent quarter (rightmost column)
- Q8 should be the oldest quarter (leftmost column)
- Column headers should be quarter names (e.g., "Mar 2024", "Dec 2023")
- Preserve negative values with minus signs
- EBITDA should be the same as Operating Profit
- All values must be exactly as displayed in the table

Return ONLY the JSON object, nothing else.
"""
    
    def _get_enhanced_annual_prompt(self) -> str:
        return """
You are a financial-data OCR specialist with expertise in extracting annual profit & loss data from Screener.in.

TASK: Extract comprehensive annual profit & loss data from the Annual Results section.

CRITICAL INSTRUCTIONS:
1. Look for the "Profit & Loss" or "Annual Results" table
2. Extract the MOST RECENT year's data (usually the rightmost column, not leftmost)
3. Also extract the column headers (year names) from the top row
4. Preserve EXACT formatting including commas, minus signs, and decimal places
5. If a value is not visible or unclear, write "NA"
6. **MANDATORY**: Return ONLY a valid JSON object. Do not include any text before or after the JSON.
7. **MANDATORY**: Do not wrap the JSON in markdown code blocks (```json or ```)

EXTRACT THE FOLLOWING DATA:
- Column Headers (Year Names) - extract from the top row of the table
- Total Revenue (Sales) - look for "Sales" or "Revenue" row
- Total Expenses - look for "Expenses" or "Total Expenses" row
- Operating Profit - look for "Operating Profit" row (this is also EBITDA)
- Net Profit - look for "Net Profit" or "PAT" row
- EBITDA - same as Operating Profit (Operating Profit = EBITDA)

OUTPUT FORMAT:
{
  "column_headers": ["Y1", "Y2", "Y3", "Y4", "Y5"],
  "total_revenue": "value",
  "total_expenses": "value",
  "operating_profit": "value",
  "net_profit": "value",
  "ebitda": "value"
}

IMPORTANT: Extract the MOST RECENT year's data (rightmost column), not the oldest year (leftmost column)

EXAMPLES:
- Column Headers: ["Mar 2024", "Mar 2023", "Mar 2022", "Mar 2021", "Mar 2020"]
- Total Revenue: "8,374" (in Crores)
- Total Expenses: "7,865" (in Crores)
- Operating Profit: "509" (in Crores)
- Net Profit: "155" (in Crores)
- EBITDA: "509" (in Crores) (same as Operating Profit)

VALIDATION RULES:
- All values should be in ₹ Crores format
- Preserve commas and minus signs exactly as displayed
- Extract the MOST RECENT year's data (rightmost column)
- Column headers should be year names (e.g., "Mar 2024", "Mar 2023")
- EBITDA should be the same as Operating Profit

Return ONLY the JSON object, nothing else.
"""
    
    def _get_enhanced_balance_sheet_prompt(self) -> str:
        return """
You are a financial-data OCR specialist with expertise in extracting balance sheet data from Screener.in.

TASK: Extract balance sheet data from the Balance Sheet section.

CRITICAL INSTRUCTIONS:
1. Look for the "Balance Sheet" table
2. Extract the most recent year's data (usually the rightmost column)
3. Preserve EXACT formatting including commas and decimal places
4. If a value is not visible or unclear, write "NA"
5. Return ONLY valid JSON format

EXTRACT THE FOLLOWING DATA:
- Total Assets - look for "Total Assets" row
- Total Liabilities - look for "Total Liabilities" row
- Net Worth (Shareholders' Equity) - look for "Reserves" or "Net Worth" row
- Working Capital - look for "Working Capital" row (if available)

OUTPUT FORMAT:
{
  "total_assets": "value",
  "total_liabilities": "value",
  "net_worth": "value",
  "working_capital": "value"
}

EXAMPLES:
- Total Assets: "12,063" (in Crores)
- Total Liabilities: "11,191" (in Crores)
- Net Worth: "872" (in Crores)
- Working Capital: "1,234" (in Crores)

VALIDATION RULES:
- All values should be in ₹ Crores format
- Preserve commas exactly as displayed
- Extract the most recent year's data (rightmost column)

Return ONLY the JSON object, nothing else.
"""
    
    def _get_enhanced_cash_flow_prompt(self) -> str:
        return """
You are a financial-data OCR specialist with expertise in extracting cash flow data from Screener.in.

TASK: Extract cash flow data from the Cash Flow section.

CRITICAL INSTRUCTIONS:
1. Look for the "Cash Flows" table
2. Extract the most recent year's data (usually the rightmost column)
3. Preserve EXACT formatting including minus signs and commas
4. If a value is not visible or unclear, write "NA"
5. Return ONLY valid JSON format

EXTRACT THE FOLLOWING DATA:
- Cash from Operating Activities - look for "Cash from Operating Activity +" row
- Cash from Investing Activities - look for "Cash from Investing Activity +" row
- Cash from Financing Activities - look for "Cash from Financing Activity +" row

OUTPUT FORMAT:
{
  "operating_cf": "value",
  "investing_cf": "value",
  "financing_cf": "value"
}

EXAMPLES:
- Operating CF: "567" (in Crores)
- Investing CF: "-104" (in Crores)
- Financing CF: "-432" (in Crores)

VALIDATION RULES:
- All values should be in ₹ Crores format
- Preserve negative values with minus signs
- Preserve commas exactly as displayed
- Extract the most recent year's data (rightmost column)

Return ONLY the JSON object, nothing else.
"""
    
    def _get_enhanced_shareholding_prompt(self) -> str:
        return """
You are a financial-data OCR specialist with expertise in extracting shareholding pattern data from Screener.in.

TASK: Extract shareholding pattern data from the Shareholding section.

CRITICAL INSTRUCTIONS:
1. Look for the "Shareholding Pattern" table
2. Extract the most recent quarter's data (usually the rightmost column)
3. Preserve EXACT percentages including decimal places
4. If a value is not visible or unclear, write "NA"
5. Return ONLY valid JSON format

EXTRACT THE FOLLOWING DATA:
- Promoter Holding (%) - look for "Promoter" or "Promoters" row
- FII Holding (%) - look for "FIIs +" or "Foreign Institutional Investors" row
- DII Holding (%) - look for "DIIs +" or "Domestic Institutional Investors" row
- Retail/Public Holding (%) - look for "Public +" or "Retail" row

OUTPUT FORMAT:
{
  "promoter_holding": "value",
  "fii_holding": "value",
  "dii_holding": "value",
  "retail_holding": "value"
}

EXAMPLES:
- Promoter Holding: "52.95%" (percentage)
- FII Holding: "29.60%" (percentage)
- DII Holding: "17.46%" (percentage)
- Retail Holding: "0.00%" (percentage)

VALIDATION RULES:
- All values should be percentages (e.g., "74.24%")
- Preserve exact decimal places
- Extract the most recent quarter's data (rightmost column)
- Sum of all holdings should be close to 100%

Return ONLY the JSON object, nothing else.
"""
    
    def _setup_driver(self) -> webdriver.Chrome:
        """Set up Chrome driver with optimized settings"""
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1440,900")
        chrome_options.add_argument("--force-device-scale-factor=2")
        chrome_options.add_argument("--disable-compress-png")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--disable-plugins")
        chrome_options.add_argument("--disable-images")
        
            driver = webdriver.Chrome(options=chrome_options)
            driver.set_page_load_timeout(30)
        return driver
    
    def _apply_optimizations(self, driver: webdriver.Chrome):
        """Apply CSS optimizations for better OCR"""
            css_optimizations = """
            <style>
            body, html { background-color: #FFFFFF !important; }
            th, td { border: 1px solid #ddd !important; padding: 4px !important; }
            * { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif !important; }
            table { background-color: #FFFFFF !important; color: #000000 !important; }
            .container, .content { padding: 20px !important; }
            </style>
            """
            driver.execute_script(f"document.head.insertAdjacentHTML('beforeend', `{css_optimizations}`);")
    
    def _detect_section_coordinates(self, driver: webdriver.Chrome, section_name: str) -> Optional[Dict]:
        """Enhanced section coordinate detection including table content"""
        try:
            # JavaScript to detect financial sections with table content
            js_script = """
            function detectFinancialSectionsWithContent() {
                const sections = {};
                const headers = document.querySelectorAll('h1, h2, h3, h4, h5, h6');
                
                console.log('Found ' + headers.length + ' headers');
                
                headers.forEach((header, index) => {
                    const text = header.textContent.toLowerCase();
                    const rect = header.getBoundingClientRect();
                    const headerY = rect.top + window.scrollY;
                    const headerHeight = rect.height;
                    
                    console.log('Header ' + index + ': ' + text + ' at y=' + headerY + ', height=' + headerHeight);
                    
                    // Map headers to section types
                    let sectionType = null;
                    
                    if (text.includes('quarterly') || text.includes('results')) {
                        sectionType = 'quarterly';
                    } else if (text.includes('profit') && text.includes('loss')) {
                        sectionType = 'annual';
                    } else if (text.includes('balance') && text.includes('sheet')) {
                        sectionType = 'balance_sheet';
                    } else if (text.includes('cash') && text.includes('flow')) {
                        sectionType = 'cash_flow';
                    } else if (text.includes('shareholding') || text.includes('share')) {
                        sectionType = 'shareholding';
                    } else if (text.includes('key') || text.includes('metrics') || 
                               text.includes('market') || text.includes('price')) {
                        sectionType = 'key_metrics';
                    }
                    
                    if (sectionType) {
                        // Find the associated table or content area
                        let contentStartY = headerY + headerHeight;
                        let contentEndY = contentStartY;
                        
                        // Look for the next header or end of page
                        let nextHeaderY = document.documentElement.scrollHeight;
                        for (let i = index + 1; i < headers.length; i++) {
                            const nextHeader = headers[i];
                            const nextRect = nextHeader.getBoundingClientRect();
                            const nextY = nextRect.top + window.scrollY;
                            if (nextY > headerY) {
                                nextHeaderY = nextY;
                                break;
                            }
                        }
                        
                        // For key metrics, look for the metrics table/box
                        if (sectionType === 'key_metrics') {
                            const metricsBox = document.querySelector('.company-info, .key-metrics, .metrics-box, .company-details');
                            if (metricsBox) {
                                const boxRect = metricsBox.getBoundingClientRect();
                                contentStartY = boxRect.top + window.scrollY;
                                contentEndY = boxRect.bottom + window.scrollY;
                            } else {
                                // Fallback: capture more content for key metrics
                                contentEndY = Math.min(headerY + 800, nextHeaderY);
                            }
                        } else {
                            // For other sections, capture content until next header or reasonable limit
                            contentEndY = Math.min(headerY + 600, nextHeaderY);
                        }
                        
                        const totalHeight = contentEndY - headerY;
                        
                        sections[sectionType] = {
                            y: headerY,
                            height: totalHeight,
                            headerHeight: headerHeight,
                            contentStartY: contentStartY,
                            contentEndY: contentEndY,
                            text: header.textContent
                        };
                        
                        console.log('Mapped to section: ' + sectionType + ' with height: ' + totalHeight);
                    }
                });
                
                return sections;
            }
            
            return detectFinancialSectionsWithContent();
            """
            
            # Execute JavaScript
            detected_sections = driver.execute_script(js_script)
            
            if detected_sections and section_name in detected_sections:
                section_data = detected_sections[section_name]
                print(f"🎯 Enhanced detection: {section_name} found at y={section_data['y']}, height={section_data['height']}")
                print(f"   📊 Content range: {section_data['contentStartY']} to {section_data['contentEndY']}")
                return section_data
            
            print(f"⚠️ Enhanced detection failed for {section_name}, using fallback")
            return None
            
        except Exception as e:
            print(f"❌ Error in enhanced detection for {section_name}: {e}")
            return None
    
    def _capture_section_screenshot(self, driver: webdriver.Chrome, section_name: str, output_dir: str) -> Optional[str]:
        """Capture screenshot with enhanced dynamic coordinate detection"""
        try:
            # Wait for content to load
            time.sleep(3)
            
            # Try enhanced dynamic coordinate detection first
            section_coords = self._detect_section_coordinates(driver, section_name)
                    
                    # Capture full screenshot
                    screenshot = driver.get_screenshot_as_png()
                    nparr = np.frombuffer(screenshot, np.uint8)
                    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
                    
                    if img is None or img.size == 0:
                return None
                    
                    # Get image dimensions
                    img_height, img_width = img.shape[:2]
                    
            # Determine cropping strategy
            if section_coords:
                # Use enhanced dynamic coordinates
                section_y = section_coords['y']
                section_height = section_coords['height']
                content_start_y = section_coords['contentStartY']
                content_end_y = section_coords['contentEndY']
                
                # Get page dimensions
                page_height = driver.execute_script("return document.documentElement.scrollHeight;")
                viewport_height = driver.execute_script("return window.innerHeight;")
                
                # Calculate scroll position to center the section in viewport
                scroll_y = max(0, section_y - (viewport_height // 2))
                driver.execute_script(f"window.scrollTo(0, {scroll_y});")
                time.sleep(1)  # Wait for scroll
                
                # Re-capture screenshot after scrolling
                screenshot = driver.get_screenshot_as_png()
                nparr = np.frombuffer(screenshot, np.uint8)
                img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
                
                # Calculate section position relative to viewport
                viewport_section_y = section_y - scroll_y
                viewport_content_start = content_start_y - scroll_y
                viewport_content_end = content_end_y - scroll_y
                
                # Use viewport-relative coordinates (accounting for device scale factor)
                    scale_factor = 2
                y_start = int(viewport_content_start * scale_factor)
                y_end = int(viewport_content_end * scale_factor)
                    
                    # Add padding and bounds checking
                padding = 30 * scale_factor
                    y_start = max(0, y_start - padding)
                y_end = min(img_height, y_end + padding)
                height = y_end - y_start
                
                if y_start >= img_height or height <= 0:
                    print(f"⚠️ Enhanced coordinates invalid for {section_name}, using fallback")
                    section_coords = None
                else:
                    cropped_img = img[y_start:y_end, :]
                    print(f"✅ Using enhanced coordinates for {section_name}: {y_start}-{y_end} (height: {height})")
            
            if not section_coords:
                # Use fallback cropping strategy
                fallback_crop = self.section_configs[section_name]['fallback_crop']
                start_y = int(img_height * fallback_crop[0])
                end_y = int(img_height * fallback_crop[1])
                cropped_img = img[start_y:end_y, :]
                print(f"📏 Using fallback crop for {section_name}: {fallback_crop[0]:.1%}-{fallback_crop[1]:.1%}")
                    
                    if cropped_img is None or cropped_img.size == 0:
                return None
                    
                    # Pre-process for OCR
                    if len(cropped_img.shape) == 3:
                        gray = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2GRAY)
                    else:
                        gray = cropped_img.copy()
                    
            # Enhanced preprocessing
                    # Despeckle and sharpen
                    despeckled = cv2.medianBlur(gray, 3)
                    gaussian = cv2.GaussianBlur(despeckled, (0, 0), 2.0)
                    sharpened = cv2.addWeighted(despeckled, 1.5, gaussian, -0.5, 0)
                    
                    # Add padding
                    processed_img = cv2.copyMakeBorder(sharpened, 20, 20, 20, 20, 
                                                    cv2.BORDER_CONSTANT, value=255)
                    
            # Save screenshot
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{section_name}_{timestamp}.png"
            filepath = os.path.join(output_dir, filename)
            
                    success = cv2.imwrite(filepath, processed_img, [cv2.IMWRITE_PNG_COMPRESSION, 0])
                    
                    if success:
                        file_size = os.path.getsize(filepath)
                print(f"✅ {section_name}: {filename} ({file_size/1024:.1f} KB)")
                return filepath
            
            return None
            
        except Exception as e:
            print(f"❌ Error capturing {section_name}: {e}")
            return None
    
    def _extract_section_data(self, section_name: str, screenshot_path: str, section_config: Dict) -> Optional[Dict]:
        """Extract data from a section screenshot using OpenAI with retry logic"""
        max_retries = 2
        
        for attempt in range(max_retries):
                    try:
                        with open(screenshot_path, "rb") as image_file:
                            image_data = base64.b64encode(image_file.read()).decode('utf-8')
                        
                # Call OpenAI
                model_name = os.getenv('OPENAI_MODEL','gpt-5')
                
                # Prepare API parameters based on model and section complexity
                api_params = {
                    "model": model_name,
                    "messages": [
                                {
                                    "role": "user",
                                    "content": [
                                {"type": "text", "text": section_config['prompt']},
                                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image_data}"}}
                                    ]
                                }
                            ],
                    # Significantly increase token limits for GPT-5 complex sections
                    "max_completion_tokens": 4000 if model_name.startswith('gpt-5') and section_name in ['quarterly', 'annual'] 
                                           else 3000 if model_name.startswith('gpt-5') 
                                           else 2000 if section_name in ['quarterly', 'annual'] 
                                           else 1500
                }
                
                # Add temperature only for non-GPT-5 models
                if not model_name.startswith('gpt-5'):
                    api_params["temperature"] = 0.1
                    
                response = self.client.chat.completions.create(**api_params)
                
                # Parse response with improved error handling for GPT-5
                        response_text = response.choices[0].message.content.strip()
                        
                # Remove markdown code blocks if present
                if response_text.startswith('```json'):
                    response_text = response_text.replace('```json', '').replace('```', '').strip()
                elif response_text.startswith('```'):
                    response_text = response_text.replace('```', '').strip()
                
                # Handle GPT-5 specific response patterns
                if model_name.startswith('gpt-5'):
                    # GPT-5 sometimes adds explanatory text before JSON
                    lines = response_text.split('\n')
                    json_start = -1
                    for i, line in enumerate(lines):
                        if line.strip().startswith('{'):
                            json_start = i
                            break
                    
                    if json_start >= 0:
                        response_text = '\n'.join(lines[json_start:])
                        
                        try:
                            data = json.loads(response_text)
                except json.JSONDecodeError as e:
                    print(f"⚠️ JSON parsing error for {section_name}: {str(e)}")
                    print(f"📝 Response preview: {response_text[:300]}...")
                    
                    # Enhanced JSON extraction patterns for GPT-5
                    json_patterns = [
                        r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',  # Simple nested JSON
                        r'\{.*?\}(?=\s*$)',  # JSON at end of response
                        r'(\{[\s\S]*\})',   # Any JSON structure (greedy)
                        r'\{.*\}',          # Fallback pattern
                    ]
                    
                    for pattern in json_patterns:
                        json_matches = re.findall(pattern, response_text, re.DOTALL)
                        for match in json_matches:
                            try:
                                data = json.loads(match)
                                print(f"✅ Successfully extracted JSON from response for {section_name} using pattern")
                                break
                        except json.JSONDecodeError:
                                continue
                        if 'data' in locals():
                            break
                            else:
                        # If all parsing attempts fail, create a fallback structure
                        print(f"❌ All JSON parsing attempts failed for {section_name}")
                        if attempt < max_retries - 1:
                            print(f"⚠️ Retry {attempt + 1} for {section_name} due to JSON parsing error")
                            time.sleep(3)  # Longer wait for GPT-5
                                continue
                        
                        # Create fallback data structure
                        data = {
                            "error": "JSON parsing failed",
                            "raw_response": response_text[:1000],  # Store more of the response
                            "extraction_status": "partial_failure",
                            "model_used": model_name
                        }
                        print(f"🔄 Using fallback data structure for {section_name}")
                        return data
                
                # Validate data
                if self._validate_section_data(data, section_name, section_config):
                    return data
                else:
                    if attempt < max_retries - 1:
                        print(f"⚠️ Retry {attempt + 1} for {section_name} due to validation failure")
                        time.sleep(1)
                        continue
                    print(f"⚠️ Failed to validate {section_name}")
                    return None
            
        except Exception as e:
                if attempt < max_retries - 1:
                    print(f"⚠️ Retry {attempt + 1} for {section_name} due to error: {e}")
                    time.sleep(1)
                    continue
                print(f"❌ Extraction failed for {section_name}: {e}")
                return None
        
        return None
    
    def _validate_section_data(self, data: Dict, section_name: str, section_config: Dict) -> bool:
        """Enhanced validation of extracted data"""
        try:
            expected_fields = section_config.get('fields', [])
            
            # Check if at least some expected fields are present
            present_fields = sum(1 for field in expected_fields if field in data)
            if present_fields == 0:
                return False
            
            # Enhanced validation for specific sections
            if section_name == 'key_metrics':
                # For key metrics, at least current_price should be present
                if 'current_price' not in data or data['current_price'] == 'NA':
                    return False
            
            elif section_name == 'quarterly':
                # For quarterly, at least revenue should be present and be an array
                if 'revenue' not in data or not isinstance(data['revenue'], list):
                            return False
                if len(data['revenue']) < 4:  # Should have at least 4 quarters
                            return False
                
                # Validate EBITDA = Operating Profit
                if 'operating_profit' in data and 'ebitda' in data:
                    if isinstance(data['operating_profit'], list) and isinstance(data['ebitda'], list):
                        if len(data['operating_profit']) == len(data['ebitda']):
                            # They should be the same
                            pass
            
            elif section_name == 'annual':
                # Validate EBITDA = Operating Profit
                if 'operating_profit' in data and 'ebitda' in data:
                    if data['operating_profit'] != data['ebitda']:
                        # They should be the same
                        pass
            
            # Validate values
            for field, value in data.items():
                if value not in ('NA', '') and value is not None:
                    if isinstance(value, list):
                        # Handle arrays (for quarterly data)
                        for item in value:
                            if item not in ('NA', '') and item is not None:
                                # Basic validation - should contain numbers
                                if not re.search(r'\d', str(item)):
                                    return False
                    else:
                        # Handle single values
                        if not re.search(r'\d', str(value)):
                            return False
            
            return True
            
        except Exception as e:
            return False
    
    def _extract_single_section(self, ticker: str, section_name: str, section_config: Dict, output_dir: str) -> Tuple[str, Optional[Dict]]:
        """Extract data from a single section with enhanced comprehensive approach"""
        driver = None
        try:
            # Clean ticker for URL
            clean_ticker = ticker.replace('.NS', '').replace('.BO', '')
            url = f"https://www.screener.in/company/{clean_ticker}/consolidated/{section_config['url_suffix']}"
            
            print(f"🌐 Loading {section_name}: {url}")
            
            # Set up driver
            driver = self._setup_driver()
            driver.get(url)
            
            # Wait for page to load
            WebDriverWait(driver, 20).until(
                lambda d: d.title and len(d.title) > 0
            )
            time.sleep(3)
            
            print(f"✅ Page loaded: {driver.title}")
            
            # Apply optimizations
            self._apply_optimizations(driver)
            
            # Capture screenshot with enhanced comprehensive approach
            screenshot_path = self._capture_section_screenshot(driver, section_name, output_dir)
            
            if not screenshot_path:
                return section_name, None
            
            # Extract data
            data = self._extract_section_data(section_name, screenshot_path, section_config)
            
            if data:
                print(f"✅ {section_name} extracted successfully")
            else:
                print(f"❌ {section_name} extraction failed")
            
            return section_name, data
            
        except Exception as e:
            print(f"❌ Error in {section_name}: {e}")
            return section_name, None
        finally:
            if driver:
                driver.quit()
    
    def extract_complete_data(self, ticker: str, output_dir: str) -> Dict[str, Any]:
        """
        Extract complete data using enhanced comprehensive approach with parallel processing
        """
        print(f"📸 Starting enhanced comprehensive Screener.in extraction for {ticker}")
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
        # Extract data from all sections in parallel with rate limiting
        extracted_data = {}
        
        with ThreadPoolExecutor(max_workers=2) as executor:  # Reduced workers to avoid rate limits
            # Submit all extraction tasks
            future_to_section = {
                executor.submit(
                    self._extract_single_section, 
                    ticker, 
                    section_name, 
                    section_config, 
                    output_dir
                ): section_name
                for section_name, section_config in self.section_configs.items()
            }
            
            # Collect results with rate limiting
            for future in as_completed(future_to_section):
                section_name, data = future.result()
                if data:
                    extracted_data[section_name] = data
                
                # Add delay between extractions to avoid rate limits
                time.sleep(0.5)
        
        # Combine data into final format
        complete_data = {
            "key_metrics": extracted_data.get('key_metrics', {}),
            "quarterly_results": extracted_data.get('quarterly', {}),
            "annual_results": extracted_data.get('annual', {}),
            "balance_sheet": extracted_data.get('balance_sheet', {}),
            "cash_flows": extracted_data.get('cash_flow', {}),
            "shareholding": extracted_data.get('shareholding', {})
        }
        
        # Calculate completeness
        total_sections = len(self.section_configs)
        filled_sections = len(extracted_data)
        completeness = (filled_sections / total_sections) * 100
        
        print(f"📊 Extraction completed: {filled_sections}/{total_sections} sections ({completeness:.1f}%)")
        
        return complete_data

def test_enhanced_extraction_v3():
    """Test the enhanced comprehensive extraction system"""
    print("🧪 Testing Enhanced Comprehensive Screener.in Extraction System V3")
    print("=" * 60)
    
    # Get OpenAI API key
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("❌ OPENAI_API_KEY not found")
        return
    
    # Initialize extractor
    extractor = EnhancedScreenerExtractionV3(api_key)
    
    # Test with DELHIVERY
    ticker = "DELHIVERY.NS"
    output_dir = "enhanced_screener_extraction_v3"
    
    # Extract complete data
    complete_data = extractor.extract_complete_data(ticker, output_dir)
    
    if complete_data:
        print(f"\n✅ Successfully extracted data for {ticker}:")
        
        # Print summary
        for section_name, section_data in complete_data.items():
            if isinstance(section_data, dict) and section_data:
                print(f"   📊 {section_name}: {len(section_data)} fields")
                
                # Show sample data
                for field, value in list(section_data.items())[:3]:
                    if isinstance(value, list):
                        print(f"      {field}: {len(value)} values")
                    else:
                        print(f"      {field}: {value}")
        
        # Save complete data
        output_file = os.path.join(output_dir, f"{ticker}_complete_data.json")
        with open(output_file, 'w') as f:
            json.dump(complete_data, f, indent=2)
        
        print(f"\n💾 Complete data saved to: {output_file}")
        
        # Print detailed analysis
        print(f"\n📈 DETAILED ANALYSIS FOR {ticker}:")
        print("=" * 50)
        
        # Key Metrics Analysis
        if complete_data.get('key_metrics'):
            km = complete_data['key_metrics']
            print(f"\n💰 KEY METRICS:")
            print(f"   Market Cap: {km.get('market_cap', 'NA')}")
            print(f"   Current Price: {km.get('current_price', 'NA')}")
            print(f"   Book Value: {km.get('book_value', 'NA')}")
            print(f"   P/E Ratio: {km.get('pe_ratio', 'NA')}")
            print(f"   ROCE: {km.get('roce', 'NA')}")
            print(f"   ROE: {km.get('roe', 'NA')}")
        
        # Quarterly Analysis
        if complete_data.get('quarterly_results'):
            qr = complete_data['quarterly_results']
            print(f"\n📊 QUARTERLY RESULTS (Last 8 quarters):")
            print(f"   Column Headers: {qr.get('column_headers', [])}")
            print(f"   Revenue: {qr.get('revenue', [])}")
            print(f"   Expenses: {qr.get('expenses', [])}")
            print(f"   Operating Profit (EBITDA): {qr.get('operating_profit', [])}")
            print(f"   Net Profit: {qr.get('net_profit', [])}")
            
            # Validate EBITDA = Operating Profit
            if 'operating_profit' in qr and 'ebitda' in qr:
                op = qr['operating_profit']
                ebitda = qr['ebitda']
                if op == ebitda:
                    print(f"   ✅ EBITDA correctly mapped to Operating Profit")
                else:
                    print(f"   ⚠️ EBITDA mapping issue: OP={op}, EBITDA={ebitda}")
        
        # Annual Analysis
        if complete_data.get('annual_results'):
            ar = complete_data['annual_results']
            print(f"\n📈 ANNUAL RESULTS:")
            print(f"   Column Headers: {ar.get('column_headers', [])}")
            print(f"   Total Revenue: {ar.get('total_revenue', 'NA')}")
            print(f"   Total Expenses: {ar.get('total_expenses', 'NA')}")
            print(f"   Operating Profit (EBITDA): {ar.get('operating_profit', 'NA')}")
            print(f"   Net Profit: {ar.get('net_profit', 'NA')}")
            
            # Validate EBITDA = Operating Profit
            if 'operating_profit' in ar and 'ebitda' in ar:
                op = ar['operating_profit']
                ebitda = ar['ebitda']
                if op == ebitda:
                    print(f"   ✅ EBITDA correctly mapped to Operating Profit")
                else:
                    print(f"   ⚠️ EBITDA mapping issue: OP={op}, EBITDA={ebitda}")
        
        # Balance Sheet Analysis
        if complete_data.get('balance_sheet'):
            bs = complete_data['balance_sheet']
            print(f"\n🏦 BALANCE SHEET:")
            print(f"   Total Assets: {bs.get('total_assets', 'NA')}")
            print(f"   Total Liabilities: {bs.get('total_liabilities', 'NA')}")
            print(f"   Net Worth: {bs.get('net_worth', 'NA')}")
            print(f"   Working Capital: {bs.get('working_capital', 'NA')}")
        
        # Cash Flow Analysis
        if complete_data.get('cash_flows'):
            cf = complete_data['cash_flows']
            print(f"\n💸 CASH FLOWS:")
            print(f"   Operating CF: {cf.get('operating_cf', 'NA')}")
            print(f"   Investing CF: {cf.get('investing_cf', 'NA')}")
            print(f"   Financing CF: {cf.get('financing_cf', 'NA')}")
        
        # Shareholding Analysis
        if complete_data.get('shareholding'):
            sh = complete_data['shareholding']
            print(f"\n👥 SHAREHOLDING PATTERN:")
            print(f"   Promoter Holding: {sh.get('promoter_holding', 'NA')}")
            print(f"   FII Holding: {sh.get('fii_holding', 'NA')}")
            print(f"   DII Holding: {sh.get('dii_holding', 'NA')}")
            print(f"   Retail Holding: {sh.get('retail_holding', 'NA')}")
        
    else:
        print("❌ Failed to extract data")

if __name__ == "__main__":
    test_enhanced_extraction_v3() 